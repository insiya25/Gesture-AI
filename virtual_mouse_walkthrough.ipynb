{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f9af7f",
   "metadata": {},
   "source": [
    "# AI Virtual Mouse \n",
    "\n",
    "In this project, we are going to create an AI based Mouse Controller. We will first detect the hand landmarks and then track and click based on these points. We will also apply smoothing techniques to make it more usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37083b45",
   "metadata": {},
   "source": [
    "I have created a python file called `HandTrackingModule.py` where it has useful functions already like the `fingersUp()` and `findDistance()` methods and these methods will allow us to very easily create this new project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de235f",
   "metadata": {},
   "source": [
    "So for this will need couple of libraries/modules such as :\n",
    "\n",
    "- mediapipe\n",
    "- numpy\n",
    "- HandTrackingModule (the one we created)\n",
    "- OpenCV\n",
    "- time\n",
    "- autopy : The autopy library is used in Python for automating tasks by controlling the keyboard, mouse, and screen. It allows you to simulate user inputs and interactions, making it useful for tasks like GUI automation, testing, and even building bots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def0188",
   "metadata": {},
   "source": [
    "So will import all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b0d85c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autopy\n",
      "  Using cached autopy-4.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-sps3kris\\autopy_0e3aef85221d4aea94dfd151b3f796eb\\setup.py\", line 8, in <module>\n",
      "      from setuptools_rust import Binding, RustExtension\n",
      "  ModuleNotFoundError: No module named 'setuptools_rust'\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install autopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a32b0b",
   "metadata": {},
   "source": [
    "For some reason autopy only supports python version 3.8 and if we downgrade our Python version then some other packages might not work, so hence will try to use another alternative which is `PyAutoGUI` and it can be installed using \n",
    "\n",
    "```python\n",
    "pip install pyautogui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cbb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import time\n",
    "import pyautogui\n",
    "from pynput.keyboard import Controller, Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0760a",
   "metadata": {},
   "source": [
    "Okay, so first thing is will run our webcam using opencv to see if its working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de6e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41042b2",
   "metadata": {},
   "source": [
    "Second thing is we have to have a fixed width and height, so we cant leave it to the default of the camera, so will change our width and height using `cap.set()`\n",
    "\n",
    "In the context of using OpenCV with the `cap.set()` function, the parameters you see—like cap.set(3, 450)—are used to set properties for the video capture object, cap.\n",
    "\n",
    "`set()`: This method is used to set a property of the video capture object.\n",
    "\n",
    "- `3`: This is a property identifier. In OpenCV, property identifiers are represented by integer values. Specifically, `3` corresponds to the `CV_CAP_PROP_FRAME_WIDTH` property, which is used to set the width of the frames captured by the video capture object, similarly for height its `4`\n",
    "- `450`: This is the value being assigned to the property. In this case, it sets the frame width to 450 pixels.\n",
    "\n",
    "`cap.set()` Adjusts the resolution of the captured frames from the camera. If the camera does not support the specified resolution, the settings may not take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f10776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ccfafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera found at index 0\n",
      "Camera found at index 1\n",
      "No camera at index 2\n",
      "No camera at index 3\n",
      "No camera at index 4\n",
      "No camera at index 5\n",
      "No camera at index 6\n",
      "No camera at index 7\n",
      "No camera at index 8\n",
      "No camera at index 9\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Try camera indices 0 through 9\n",
    "for i in range(10):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Camera found at index {i}\")\n",
    "        cap.release()  # Release the camera\n",
    "    else:\n",
    "        print(f\"No camera at index {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbeb18",
   "metadata": {},
   "source": [
    "Now will simply write our code for capturing frames from the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5daf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    '''\n",
    "    This reads a frame (image) from the video capture. \n",
    "    success is True if the frame is read correctly, and frame is the actual image.\n",
    "    '''\n",
    "    success, frame = cap.read()\n",
    "    '''This displays the captured frame in a window titled \"image\".'''\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    \n",
    "    # Resize the window\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    \n",
    "    '''\n",
    "    cv2.waitKey(10): This waits for 10 milliseconds for a key press. \n",
    "    It checks if a key is pressed during that time.\n",
    "    & 0xFF: This ensures that the result is within the range of valid key codes, \n",
    "    as some systems may return more than 8 bits.\n",
    "    == ord(\"q\"): This checks if the key pressed is the letter \"q\". \n",
    "    The ord(\"q\") function gets the ASCII value of the character \"q\".\n",
    "    '''\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        '''If the key pressed is \"q\", this command exits the loop.'''\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad5c09",
   "metadata": {},
   "source": [
    "So that is all good!\n",
    "\n",
    "Now next what we can do is add our Detector for hand tracking, first lets discuss the steps we are going to take to create this project:\n",
    "\n",
    "1. **Find hand landmarks :** This is the first step\n",
    "2. **Get the tip of the index and middle fingers :** Second step is we want to get the tip of the index and the middle finger, so the idea is, if we have just the index finger up then the mouse cursor will move, if we also have the middle finger up then it will be in clicking mode, we also need to check the distance between two fingers so if the distance is less than a certain value then will detect it as a **click**, so u can bring ur fingers together and then click, this will be **clicking mode** and in this mode u wont be able to move the cursor unless u put ur middle finger down so ur in **moving mode**\n",
    "3. **Check which fingers are up :** So once we have the tip of both fingers, we will check which fingers are up\n",
    "4. **Only index finger (moving mode) :** Then based on the information, we will check if its in moving mode (index mode)\n",
    "5. **Convert Coordinates :** And if it is in moving mode, then we are going to convert our Coordinates, now why do we need to convert? Because our Webcam will give us a value of lets say 640 to 480, the one who made this tutorial, his screen in in HD 920 by 1080, so we need to convert these coordinates so that we get the correct positioning\n",
    "6. **Smoothen Values :** Then will add another step to **Smoothen** the values, why do we need to do that? So that the cursor is not very jittery or doesnt flicker alot\n",
    "7. **Move Mouse :** Once smoothen is done we can simply move our mouse\n",
    "8. **Both Index and Middle fingers are up then Clicking mode :** Then we need to check when we are in clicking mode, so when both the fingers are up then it is in Clicking mode\n",
    "9. **Find distance between fingers :** So then we find the distance between these fingers\n",
    "10. **Click mouse if distance is short :** Then if the distance is short then we are going to CLICK\n",
    "11. **Frame Rate :** Checking the frame rates, the 11 and 12 steps are pretty easy \n",
    "12. **Dislay :** We have already done the 12th display step since its just to display/render whats happening\n",
    "\n",
    "These steps might seems alot but some of them are actually easy, some of them are single lines so dont worry about these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d8bdd",
   "metadata": {},
   "source": [
    "Now we can first go on with the frame rates as its very simple, we can simply get it using `time` module and display it on the screen using `cv2.putText()` which takes in the `frames`, the `text` which in our case is `fps`, `coordinates position`, cv2 fonts from `cv2.FONT_HERSHEY_PLAIN`, The thicken which we put as `3` then simply the color then again `3` which is also the thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b18ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(\n",
    "        frame, # image to put text on\n",
    "        str(int(fps)), # the text\n",
    "        (20, 50), # position\n",
    "        cv2.FONT_HERSHEY_PLAIN, # font\n",
    "        3, # scale/size of font\n",
    "        (255, 0, 0), # color\n",
    "        3 # thickness\n",
    "    )\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b8dff",
   "metadata": {},
   "source": [
    "So if we run the above cell u should see your FPS show up on your webcam.\n",
    "\n",
    "So now we have the fps and is displaying them as well, now what we will do is the rest of the steps.\n",
    "\n",
    "So first of all we have to get the landmarks, to get the landmark we have to declare the detector on top, we have to create the object of the detector from `HandTrackingModule` module,\n",
    "\n",
    "And inside the detector we can add like the maximum hands, so we are only expecting one hand so will pass `maxHands=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "424d0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281ccf5",
   "metadata": {},
   "source": [
    "Then inside the loop we will apply `findHands()` from the detector to the frame, \n",
    "\n",
    "then will find the position of this hand, we can do it by \n",
    "\n",
    "        lmlist, bbox = detector.findPosition(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab485d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2a816",
   "metadata": {},
   "source": [
    "So when u run above code, it should start Detecting your hand, it would start detecting the bounding box, the fingers and the landmarks as well, so thats pretty good. so that was our step 1 and we are done with it, \n",
    "\n",
    "Now we will checkl that if our length of the `lmlist` is not equal to 0, then we are going to get the TIP info, so we are getting `x1` and `y1` which is just the points of the index finger.\n",
    "\n",
    "Similarly will do the same for the middle finger and store it in `x2` and `y2`\n",
    "\n",
    "So these will give us the coordinates of our Index and Middle fingers\n",
    "\n",
    "We do not to draw this at this point, we can just print them out if u want, we can just `print(x1, y1, x2, y2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768b840b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611 386 565 385\n",
      "575 308 525 299\n",
      "574 292 523 278\n",
      "529 185 486 184\n",
      "532 166 467 132\n",
      "478 103 409 74\n",
      "443 72 380 50\n",
      "426 58 369 39\n",
      "424 59 367 41\n",
      "420 62 363 46\n",
      "418 63 360 48\n",
      "414 68 358 51\n",
      "411 71 356 54\n",
      "412 75 360 56\n",
      "415 76 361 57\n",
      "416 76 358 57\n",
      "415 76 346 73\n",
      "407 83 369 173\n",
      "408 84 375 211\n",
      "415 87 394 266\n",
      "417 89 396 269\n",
      "418 89 393 274\n",
      "417 90 393 277\n",
      "417 90 391 277\n",
      "418 89 390 280\n",
      "417 88 390 280\n",
      "420 89 394 279\n",
      "423 89 392 274\n",
      "434 89 388 251\n",
      "452 95 379 141\n",
      "453 111 378 80\n",
      "453 115 380 78\n",
      "448 133 394 78\n",
      "454 153 406 86\n",
      "455 193 428 141\n",
      "455 198 430 159\n",
      "474 197 445 170\n",
      "465 211 452 130\n",
      "470 228 448 93\n",
      "474 222 444 87\n",
      "474 223 443 82\n",
      "479 218 446 79\n",
      "465 217 460 83\n",
      "497 136 460 73\n",
      "498 132 458 71\n",
      "500 118 458 74\n",
      "520 118 472 80\n",
      "570 155 523 114\n",
      "896 524 926 573\n"
     ]
    }
   ],
   "source": [
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        print(x1, y1, x2, y2)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c21bca",
   "metadata": {},
   "source": [
    "You can see above its printing the points of the fingers, u can put any of the fingers up and down or both and see it works perfectly fine!\n",
    "\n",
    "Okay so this is good! we are done with the step 2\n",
    "\n",
    "Now will go onto the third part which is to Check which finger are up, now these is exremely simple because we have already created a method in the  `HandTrackingModule` module by the name of `fingersUp()`, all we have to do is we have to call it and simply print it to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69702b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[0, 1, 1, 1, 1]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 0]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 0, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        print(fingers)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6e2e7",
   "metadata": {},
   "source": [
    "When u run the code above u will see a One hot encoded kind of array, of 0 and 1, if u lift first finger up, it will be 1, else 0, same with the other fingers, its pretty cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514ad4e",
   "metadata": {},
   "source": [
    "Now lets go to the next step which is step 4, So now we need to check if only the index finger is up, so we can write if fingers at index `[1]` (which is the index finger) is equal to 1 (means its up) and fingers at index `[2]` (mid finger) is equal to 0 (meaning its down), \n",
    "\n",
    "So basically this is when the index finger is up and the middle finger is down, so this will be the moving mode, so here we need to check wherre our fingers is moving so we get those points and we send it to the mouse cursor\n",
    "\n",
    "Also before this, we need to do step 5 and convert the coordinates, we need to convert 1 range to another range using `np.interp()`, in here we want to convert `x1` values from range of 0 to the width of our webcam and the second range is from 0 to Width of the screen, and same thing we will do for the height, will store both in `x3` and `y3`\n",
    "\n",
    "But we havent gotten the width and height of our screen, so in order to get the exact value, we will define them up using `pyautogui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247766e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1366, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5569b8",
   "metadata": {},
   "source": [
    "So now that we have these values we can continue and say "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f54c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (0, WIDTH_CAM), (0, width_screen))\n",
    "            y3 = np.interp(y1, (0, HEIGHT_CAM), (0, height_screen))\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee84316",
   "metadata": {},
   "source": [
    "So `x3` and `y3` are the points that now we have converted, and now we will send these values to the Mouse, we will do step 6 of Smoothening the value later once we see whats the original result and then we can convert it, anyways now will do step 7 using **pyautogui**\n",
    "\n",
    "        # Move the mouse to the (x3, y3) coordinates\n",
    "        pyautogui.moveTo(x3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f84f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (0, WIDTH_CAM), (0, width_screen))\n",
    "            y3 = np.interp(y1, (0, HEIGHT_CAM), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaac0fe",
   "metadata": {},
   "source": [
    "If u run above code u will see now u can move the mouse with the index finger which is sexy!\n",
    "\n",
    "But u might get the problem where when u move to left the cursor would go right, so this is very annoying, so to fix it what will do is simply flip it\n",
    "\n",
    "So in order to flip it, we just need to flip the width, so we can say whatever the width of the screen is minus it with `x3`\n",
    "\n",
    "        pyautogui.moveTo(width_screen - x3, y3)\n",
    "          \n",
    "Now u can run and try again, but since its already good for us we dont have to do it :p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4ecac",
   "metadata": {},
   "source": [
    "So this is good enough, now what we can do is, we can draw a Circle on the tip of the finger of our Index finger so that we know we are moving the mouse\n",
    "\n",
    "We can do it whenever we are in moving mode using \n",
    "\n",
    "```python\n",
    "cv2.circle(frame,\n",
    "           (x1, y1), # this is where we wanna draw \n",
    "           15, # this is the radius\n",
    "           (255, 0, 255), # the color of the circle\n",
    "           cv2.FILLED # filling the circle\n",
    "```\n",
    "\n",
    "So now when ur index finger is up it meaning ur in moving mode, it should draw the purple circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715cb0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (0, WIDTH_CAM), (0, width_screen))\n",
    "            y3 = np.interp(y1, (0, HEIGHT_CAM), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a21a95",
   "metadata": {},
   "source": [
    "The error you're seeing, FailSafeException, is a built-in feature of PyAutoGUI designed to prevent automation from running out of control. If the mouse is moved to one of the corners of the screen (usually the top-left corner), PyAutoGUI will raise a FailSafeException and stop execution to ensure the user can regain control.\n",
    "\n",
    "If you want to disable this fail-safe (though not recommended for safety reasons), you can set the pyautogui.FAILSAFE option to False. This removes the stop-trigger when the mouse is moved to a corner.\n",
    "\n",
    "```python\n",
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc2f47",
   "metadata": {},
   "source": [
    "Anyways, now one of the main issues here is that, when we move the index finger to the edges especially when we wanna go all the way down, its pretty bad because the hand is not detected properly\n",
    "\n",
    "So what we can do is, we can set Region where we want to detect the movements, so instead of the whole frame size we can set a particular range, so how can we do that?\n",
    "\n",
    "First of all lets create that range using `cv2.rectangle()`, we can define `frame_reduction` as variable above then do this\n",
    "\n",
    "```python\n",
    "cv2.rectangle(\n",
    "    frame,\n",
    "    (frame_reduction, frame_reduction), # top-left corner of the rectangle\n",
    "    (WIDTH_CAM - frame_reduction, HEIGHT_CAM - frame_reduction), # bottom-right corner\n",
    "    (255, 0, 255), # color of rectangle\n",
    "    2 # thickness\n",
    ")\n",
    "```\n",
    "\n",
    "So this will draw a Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69f40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "frame_reduction = 100\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:]\n",
    "        x2, y2 = lmlist[12][1:]\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (0, WIDTH_CAM), (0, width_screen))\n",
    "            y3 = np.interp(y1, (0, HEIGHT_CAM), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c39b4",
   "metadata": {},
   "source": [
    "So if u run above code u will see a rectangle, Okay now the idea here is that, for example when my index finger reaches the top of the rectangle, then it should also reach the top of the screen, similarly for down, left and right, so this way our hand will stay in detection range and we could also move the cursor till the edges\n",
    "\n",
    "So how do we reflect this on our `x3` and `y3`? \n",
    "\n",
    "So all we have to do is, its very simple, make changes here, instead of `0` write `frame_reduction` and `WIDTH_CAM - frame_reduction` and `HEIGHT_CAM - frame_reduction` and thats it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4437c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "frame_reduction = 100\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:] # 8 is the point for index finger\n",
    "        x2, y2 = lmlist[12][1:] # 12 is for middle finger\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (frame_reduction, WIDTH_CAM-frame_reduction), (0, width_screen))\n",
    "            y3 = np.interp(y1, (frame_reduction, HEIGHT_CAM-frame_reduction), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6107e",
   "metadata": {},
   "source": [
    "So if we run the above code, we will be able to move the Mouse cursor till the edges as we move our finger to the edges of the Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e50cd",
   "metadata": {},
   "source": [
    "Now moving next to detect the clicks, so we will do step 8 now where when both the index and middle finger are up, so when both are up then we need to fiind distance between our fingers, so we can do it using `detector.findDistance()`, and we can simply pass `8` which is point for index finger and point `12` which is of the mid finger, these are just landmarks id uses in Mediapipe model i guess.\n",
    "\n",
    "Then will simply unpack the values and it will return us `length`, `frame` and `lineInfo`\n",
    "\n",
    "The main thing we need is the length, now we need to check the length between the 2 fingers, lets simply print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4477238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.00714249274856\n",
      "67.23094525588644\n",
      "60.83584469702052\n",
      "62.177166226839255\n",
      "59.16924876994806\n",
      "52.478567053607705\n",
      "42.80186911806539\n",
      "38.948684188300895\n",
      "38.2099463490856\n",
      "40.36087214122113\n",
      "40.45985664828782\n",
      "43.139309220245984\n",
      "41.72529209005013\n",
      "40.311288741492746\n",
      "41.72529209005013\n",
      "41.72529209005013\n",
      "41.036569057366385\n",
      "41.012193308819754\n",
      "40.311288741492746\n",
      "40.36087214122113\n",
      "41.773197148410844\n",
      "41.773197148410844\n",
      "41.72529209005013\n",
      "39.59797974644666\n",
      "38.18376618407357\n",
      "37.48332962798263\n",
      "41.72529209005013\n",
      "38.18376618407357\n",
      "36.87817782917155\n",
      "38.948684188300895\n",
      "37.64306044943742\n",
      "40.311288741492746\n",
      "36.76955262170047\n",
      "37.64306044943742\n",
      "38.28837943815329\n",
      "38.28837943815329\n",
      "36.796738985948195\n",
      "36.235341863986875\n",
      "36.124783736376884\n"
     ]
    }
   ],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "frame_reduction = 100\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:] # 8 is the point for index finger\n",
    "        x2, y2 = lmlist[12][1:] # 12 is for middle finger\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (frame_reduction, WIDTH_CAM-frame_reduction), (0, width_screen))\n",
    "            y3 = np.interp(y1, (frame_reduction, HEIGHT_CAM-frame_reduction), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "            \n",
    "        # 8. Both index and mid fingers are up: Clicking mode\n",
    "        if fingers[1] == 1 and fingers[2] == 1:\n",
    "            length, frame, lineInfo = detector.findDistance(8, 12, frame)\n",
    "            print(length)\n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e2a2d",
   "metadata": {},
   "source": [
    "You can see it not only draw a center point which is its giving us indication of both fingers being up and it also printed the length between the 2 fingers\n",
    "\n",
    "So what we can do next is, we can check, if the length is below a certain value, then we will detect it as a click, but first we need to define the Treshold, u can try running above code again and close both ur finger together and see what value it prints for the length, so we will use that as a treshold\n",
    "\n",
    "So well it looks like 40-45, lets just set the treshold to be 40, so we can say if the length is less than 40 then will detect it as a click, also when clicking will draw the same center circle but just with a different color (green) so we know that it has been clicked and will use pyautogui for the clicking\n",
    "\n",
    "To draw that middle circle and recolor it, will simply use `lineInfo[4]` and `lineInfo[5]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f8ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "frame_reduction = 100\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:] # 8 is the point for index finger\n",
    "        x2, y2 = lmlist[12][1:] # 12 is for middle finger\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (frame_reduction, WIDTH_CAM-frame_reduction), (0, width_screen))\n",
    "            y3 = np.interp(y1, (frame_reduction, HEIGHT_CAM-frame_reduction), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "            \n",
    "        # 8. Both index and mid fingers are up: Clicking mode\n",
    "        if fingers[1] == 1 and fingers[2] == 1:\n",
    "            length, frame, lineInfo = detector.findDistance(8, 12, frame)\n",
    "            #print(length)\n",
    "            if length < 40:\n",
    "                cv2.circle(frame, (lineInfo[4], lineInfo[5]), 15, (0,255,255), cv2.FILLED)\n",
    "            \n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74641a80",
   "metadata": {},
   "source": [
    "So if u run the above code and put both index and mid finger up, it will recolor the center circle when u close them together\n",
    "\n",
    "So now we actually need to do the clicking, hence will use `pyaugogui`, its very easy and simple\n",
    "\n",
    "    # Perform a mouse click at the current position\n",
    "    pyautogui.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4515f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "frame_reduction = 100\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x1, y1 = lmlist[8][1:] # 8 is the point for index finger\n",
    "        x2, y2 = lmlist[12][1:] # 12 is for middle finger\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (frame_reduction, WIDTH_CAM-frame_reduction), (0, width_screen))\n",
    "            y3 = np.interp(y1, (frame_reduction, HEIGHT_CAM-frame_reduction), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(x3, y3)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "            \n",
    "        # 8. Both index and mid fingers are up: Clicking mode\n",
    "        if fingers[1] == 1 and fingers[2] == 1:\n",
    "            # 9. Find distance between index and mid finger\n",
    "            length, frame, lineInfo = detector.findDistance(8, 12, frame)\n",
    "            #print(length)\n",
    "            \n",
    "            # 10. Click mouse if the distance is short\n",
    "            if length < 40:\n",
    "                cv2.circle(frame, (lineInfo[4], lineInfo[5]), 15, (0,255,255), cv2.FILLED)\n",
    "                pyautogui.click()\n",
    "            \n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d3b5c",
   "metadata": {},
   "source": [
    "Perfect! if u run above code we can see the clicking does work\n",
    "\n",
    "Now the problem is its very jittery and shaky u can hardly control it, so its a vert big problem which is not allowing us to use this Virtual Mouse properly, so what can we do? Well we can do step 6 now and Smoothen the values, so how can we do that?\n",
    "\n",
    "What we can do is, instead of sending exactly the same value, we will dilute it a lil bit, so we will smoothen it so it goes step by step\n",
    "\n",
    "First of all we will create a variable called `smoothening` and give it a value lets say 5, its a random value, we can tweak it and see which gives more smoothening\n",
    "\n",
    "We also need to create couple of more variables\n",
    "\n",
    "`plocX` is basically previous location of X\n",
    "\n",
    "`plocy` is previous location of y\n",
    "\n",
    "`curlocX` is current location of X\n",
    "\n",
    "`curlocy` is current location of y\n",
    "\n",
    "We can put all 4 values as 0\n",
    "\n",
    "So now what we will do is, use this value and update them each iteration to smoothen our mouse movements, we can use the formula of\n",
    "\n",
    "    current location of X = prev loc of X + (x3 - prev loc of x) / smoothening value\n",
    "    current location of y = prev loc of y + (y3 - prev loc of y) / smoothening value\n",
    "\n",
    "\n",
    "Now we can simply pass these values to `pyautogui.moveTo()`\n",
    "\n",
    "Then will simply update prev loc of x to be curr loc of x and prev loc of y to be curr loc of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a96dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the keyboard controller\n",
    "keyboard = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48ba24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable fail-safe (not recommended)\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "detector = htm.handDetector(\n",
    "    maxHands=1\n",
    ")\n",
    "\n",
    "smoothening = 3\n",
    "plocX, plocy = 0,0\n",
    "curlocX, curlocy = 0,0\n",
    "\n",
    "frame_reduction = 50\n",
    "\n",
    "\n",
    "\n",
    "# getting screen width and height\n",
    "width_screen, height_screen =  pyautogui.size()\n",
    "width_screen, height_screen\n",
    "\n",
    "previous_time = 0\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "WIDTH_CAM = 640\n",
    "HEIGHT_CAM = 480\n",
    "cap.set(3, WIDTH_CAM)\n",
    "cap.set(4, HEIGHT_CAM)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    # 1. find hand landmarks\n",
    "    frame = detector.findHands(frame) \n",
    "    lmlist, bbox = detector.findPosition(frame)\n",
    "    \n",
    "    # 2. Get the tip of the index and middle fingers\n",
    "    if len(lmlist) != 0:\n",
    "        x0, y0 = lmlist[4][1:]  # 4 for Thumb tip\n",
    "        x1, y1 = lmlist[8][1:] # 8 is the point for index finger\n",
    "        x2, y2 = lmlist[12][1:] # 12 is for middle finger\n",
    "        # print(x1, y1, x2, y2)\n",
    "        \n",
    "        # 3. Check which fingers are up\n",
    "        fingers = detector.fingersUp()\n",
    "        \n",
    "        cv2.rectangle(frame, (frame_reduction, frame_reduction),\n",
    "                        (WIDTH_CAM - frame_reduction,\n",
    "                        HEIGHT_CAM - frame_reduction),\n",
    "                        (255, 0, 255),\n",
    "                        2 \n",
    "                     )\n",
    "        \n",
    "        # 4. Only Index Finger : Moving Mode\n",
    "        if fingers[1]==1  and fingers[2]==0:\n",
    "            \n",
    "            # 5. Convert Coordinates\n",
    "            x3 = np.interp(x1, (frame_reduction, WIDTH_CAM-frame_reduction), (0, width_screen))\n",
    "            y3 = np.interp(y1, (frame_reduction, HEIGHT_CAM-frame_reduction), (0, height_screen))\n",
    "            \n",
    "            # 6. Smoothen Values\n",
    "            curlocX = plocX + (x3 - plocX) / smoothening\n",
    "            curlocy = plocy + (y3 - plocy) / smoothening\n",
    "            \n",
    "            # 7. Move Mouse\n",
    "            pyautogui.moveTo(curlocX, curlocy)\n",
    "            cv2.circle(frame, (x1, y1), 15, (255,0,255), cv2.FILLED)\n",
    "            plocX, plocy = curlocX, curlocy\n",
    "            \n",
    "        # 8. Both index and mid fingers are up: Clicking mode\n",
    "        if fingers[1] == 1 and fingers[2] == 1:\n",
    "            # 9. Find distance between index and mid finger\n",
    "            length, frame, lineInfo = detector.findDistance(8, 12, frame)\n",
    "            #print(length)\n",
    "            \n",
    "            # 10. Click mouse if the distance is short\n",
    "            if length < 40:\n",
    "                cv2.circle(frame, (lineInfo[4], lineInfo[5]), 15, (0,255,255), cv2.FILLED)\n",
    "                pyautogui.click()\n",
    "                \n",
    "            \n",
    "    \n",
    "    # 11. Frame Rate\n",
    "    current_time = time.time()\n",
    "    fps = 1/(current_time - previous_time)\n",
    "    previous_time = current_time\n",
    "    cv2.putText(frame, str(int(fps)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 12. Dislay\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.resizeWindow(\"image\", WIDTH_CAM, HEIGHT_CAM)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3ee75",
   "metadata": {},
   "source": [
    "WOW! as u can see it works very well!\n",
    "\n",
    "All of this is possible thanks to the `HandTrackingModule` module, if we didnt have this and it would be very difficult and take quite alot of time to create such project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d24b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
